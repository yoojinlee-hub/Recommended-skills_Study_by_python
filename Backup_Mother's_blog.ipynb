{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfvDg3CqO7DzwUrkQgubV0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoojinlee-hub/Recommended-skills_Study_by_python/blob/main/Backup_Mother's_blog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT code"
      ],
      "metadata": {
        "id": "NSLSkw0nD1B5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu6Kb6sXDkFn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# 크롤링할 페이지 URL\n",
        "url = 'https://blog.aladin.co.kr/nama?CommunityType=AllView&page=1&cnt=1598'\n",
        "page_number = 1\n",
        "data = []\n",
        "\n",
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while page_number <= 320:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch page {page_number}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # 원하는 데이터를 추출하여 data 리스트에 저장\n",
        "    # 이 예제에서는 페이지의 제목을 가져오도록 했습니다. 원하는 데이터에 따라 수정할 수 있습니다.\n",
        "    titles = soup.select('.title')\n",
        "    for title in titles:\n",
        "        data.append(title.text)\n",
        "\n",
        "    page_number += 1\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame(data, columns=['Title'])\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "z9XJgHrSDrZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Actual Code(From ME)"
      ],
      "metadata": {
        "id": "8Lx3Wp-vD36y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "54FqbCUkDqP4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 크롤링할 페이지 URL\n",
        "url = 'https://blog.aladin.co.kr/nama?CommunityType=AllView&page=1&cnt=1598'\n",
        "page_number = 1\n",
        "image_data = []\n",
        "text_data = []"
      ],
      "metadata": {
        "id": "4qLSGLNpDsYC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while page_number <= 320:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch page {page_number}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    is_it_going = 0\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "        print(is_it_going)\n",
        "        is_it_going += 1\n",
        "\n",
        "    page_number += 1\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n"
      ],
      "metadata": {
        "id": "1cWhMhCfDvPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data.xlsx', index=False)"
      ],
      "metadata": {
        "id": "ZG2YkFlADyQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "RETRY\n"
      ],
      "metadata": {
        "id": "gA5-MQ3KRMWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install retry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmtXYIDmRVY-",
        "outputId": "e946c1bf-63f3-4496-be75-e3060389cbf7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting retry\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py, retry\n",
            "Successfully installed py-1.11.0 retry-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from retry import retry\n",
        "\n",
        "# 크롤링할 페이지 URL\n",
        "url = 'https://blog.aladin.co.kr/nama?CommunityType=AllView&page=1&cnt=1598'\n",
        "page_number = 1\n",
        "image_data = []\n",
        "text_data = []\n",
        "\n",
        "# 함수를 최대 3번까지 재시도하도록 설정\n",
        "@retry(tries=3, delay=1)\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return response\n",
        "\n",
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while page_number <= 320:\n",
        "    try:\n",
        "        response = fetch_page(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page {page_number}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "\n",
        "    page_number += 1\n",
        "    print(page_number)\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knoRt1dMRLR0",
        "outputId": "a88764ad-6022-40d0-efc7-4190809ec5ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n"
          ]
        }
      ]
    }
  ]
}