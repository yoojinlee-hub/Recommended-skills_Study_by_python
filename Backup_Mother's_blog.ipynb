{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQMzu+HzotNxmjzNWJAHGJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoojinlee-hub/Recommended-skills_Study_by_python/blob/main/Backup_Mother's_blog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT code"
      ],
      "metadata": {
        "id": "NSLSkw0nD1B5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iu6Kb6sXDkFn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# 크롤링할 페이지 URL\n",
        "url = 'https://blog.aladin.co.kr/nama?CommunityType=AllView&page=1&cnt=1598'\n",
        "page_number = 1\n",
        "data = []\n",
        "\n",
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while page_number <= 320:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch page {page_number}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # 원하는 데이터를 추출하여 data 리스트에 저장\n",
        "    # 이 예제에서는 페이지의 제목을 가져오도록 했습니다. 원하는 데이터에 따라 수정할 수 있습니다.\n",
        "    titles = soup.select('.title')\n",
        "    for title in titles:\n",
        "        data.append(title.text)\n",
        "\n",
        "    page_number += 1\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame(data, columns=['Title'])\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "z9XJgHrSDrZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Actual Code(From ME)"
      ],
      "metadata": {
        "id": "8Lx3Wp-vD36y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "54FqbCUkDqP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 크롤링할 페이지 URL\n",
        "url = 'https://blog.aladin.co.kr/nama?CommunityType=AllView&page=1&cnt=1598'\n",
        "page_number = 1\n",
        "image_data = []\n",
        "text_data = []"
      ],
      "metadata": {
        "id": "4qLSGLNpDsYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while page_number <= 320:\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch page {page_number}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    is_it_going = 0\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "        print(is_it_going)\n",
        "        is_it_going += 1\n",
        "\n",
        "    page_number += 1\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n"
      ],
      "metadata": {
        "id": "1cWhMhCfDvPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data.xlsx', index=False)"
      ],
      "metadata": {
        "id": "ZG2YkFlADyQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "RETRY\n"
      ],
      "metadata": {
        "id": "gA5-MQ3KRMWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install retry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmtXYIDmRVY-",
        "outputId": "32278bac-8dcc-4295-bd96-af1e1b832ab3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting retry\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py, retry\n",
            "Successfully installed py-1.11.0 retry-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from retry import retry\n",
        "\n",
        "# 크롤링할 페이지 URL\n",
        "url = 'https://blog.aladin.co.kr/nama?CommunityType=AllView&page=1&cnt=1598'\n",
        "page_number = 1\n",
        "image_data = []\n",
        "text_data = []\n",
        "\n",
        "# 함수를 최대 3번까지 재시도하도록 설정\n",
        "@retry(tries=3, delay=1)\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return response"
      ],
      "metadata": {
        "id": "qpeTIXMiTH98"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from retry import retry\n",
        "\n",
        "# 크롤링할 페이지 URL\n",
        "url = 'https://blog.aladin.co.kr/nama?CommunityType=AllView&page=1&cnt=1598'\n",
        "page_number = 1\n",
        "image_data = []\n",
        "text_data = []\n",
        "\n",
        "# 함수를 최대 3번까지 재시도하도록 설정\n",
        "@retry(tries=3, delay=1)\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return response\n",
        "\n",
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while page_number <= 50:\n",
        "    try:\n",
        "        response = fetch_page(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page {page_number}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "\n",
        "    page_number += 1\n",
        "    print(page_number)\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knoRt1dMRLR0",
        "outputId": "16d600e4-5a9d-4513-bf97-0e4ce33f9082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while 50 < page_number <= 100:\n",
        "    try:\n",
        "        response = fetch_page(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page {page_number}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "\n",
        "    page_number += 1\n",
        "    print(page_number)\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data2.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJIW8i5IS2DJ",
        "outputId": "721504d8-974c-48ea-f131-c4a857a50554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while 100 < page_number <= 150:\n",
        "    try:\n",
        "        response = fetch_page(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page {page_number}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "\n",
        "    page_number += 1\n",
        "    print(page_number)\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data3.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "L16Q8wwoTS0r"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while 150 < page_number <= 200:\n",
        "    try:\n",
        "        response = fetch_page(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page {page_number}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "\n",
        "    page_number += 1\n",
        "    print(page_number)\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data4.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "d3ZkSUMzTXUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while 200 < page_number <= 250:\n",
        "    try:\n",
        "        response = fetch_page(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page {page_number}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "\n",
        "    page_number += 1\n",
        "    print(page_number)\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data5.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "eNMq08dPTayl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while 250 < page_number <= 300:\n",
        "    try:\n",
        "        response = fetch_page(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page {page_number}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "\n",
        "    page_number += 1\n",
        "    print(page_number)\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data6.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "lNFVxjR2Td7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while 300 < page_number <= 320:\n",
        "    try:\n",
        "        response = fetch_page(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page {page_number}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        image_text = [img['src'] for img in images]\n",
        "        text = entry.get_text().strip()\n",
        "        image_data.append(image_text)\n",
        "        text_data.append(text)\n",
        "\n",
        "    page_number += 1\n",
        "    print(page_number)\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "# 데이터를 Excel 파일로 저장\n",
        "df.to_excel('aladin_blog_data7.xlsx', index=False)\n"
      ],
      "metadata": {
        "id": "cYVuxKSeTkVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zhsD3SqaTY2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# 함수를 최대 3번까지 재시도하도록 설정\n",
        "@retry(tries=3, delay=1)\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    return response\n",
        "\n",
        "# 크롤링할 페이지 URL\n",
        "url = 'https://blog.aladin.co.kr/nama?CommunityType=AllView&page=1&cnt=1598'\n",
        "page_number = 1\n",
        "image_data = []\n",
        "text_data = []\n",
        "\n",
        "# 페이지 번호가 320이 될 때까지 반복\n",
        "while page_number <= 320:\n",
        "    try:\n",
        "        response = fetch_page(url)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to fetch page {page_number}: {e}\")\n",
        "        break\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # div class=\"entry paper-contents\" 내부의 이미지와 텍스트 추출\n",
        "    entries = soup.select('.entry.paper-contents')\n",
        "    for entry in entries:\n",
        "        images = entry.find_all('img')\n",
        "        text = entry.get_text().strip()\n",
        "\n",
        "        # 이미지를 binary로 저장\n",
        "        for i, img in enumerate(images):\n",
        "            img_url = img['src']\n",
        "            img_data = requests.get(img_url).content\n",
        "            image_data.append(img_data)\n",
        "\n",
        "            # 이미지를 별도의 디렉토리에 저장\n",
        "            image_dir = 'images/'\n",
        "            os.makedirs(image_dir, exist_ok=True)\n",
        "            with open(os.path.join(image_dir, f'image_{i}.jpg'), 'wb') as f:\n",
        "                f.write(img_data)\n",
        "\n",
        "        text_data.append(text)\n",
        "\n",
        "    page_number += 1\n",
        "    print(page_number)\n",
        "    url = f'https://blog.aladin.co.kr/nama?CommunityType=AllView&page={page_number}&cnt=1598'\n",
        "\n",
        "# 데이터를 DataFrame으로 변환\n",
        "df = pd.DataFrame({'Images': image_data, 'Text': text_data})\n",
        "\n",
        "df.to_excel('aladin_blog_data.xlsx', index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN8yOokbTpU4",
        "outputId": "24bdb274-dfc4-4016-96f6-c74df1791b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    }
  ]
}